{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from hydra import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f7351a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_analysis(base_path, single_test=False):\n",
    "    paths = list(\n",
    "        filter(lambda x: x is not None,\n",
    "        [path if path.is_dir() else None for path in Path(base_path).iterdir()]))\n",
    "\n",
    "    if single_test:\n",
    "        paths = [Path(base_path)]\n",
    "    \n",
    "    file = pd.read_csv(paths[0] / 'pred.csv').file\n",
    "\n",
    "    cols = [\"file\"]\n",
    "    cols.extend([f\"prognosis_{ind}\" for ind in range(len(list(paths)))])\n",
    "    preds = pd.DataFrame(columns=cols)\n",
    "\n",
    "    preds[\"file\"] = file\n",
    "\n",
    "    for ind, path_dir in enumerate(paths):\n",
    "        try:\n",
    "            pred = pd.read_csv(path_dir / 'pred.csv')\n",
    "            preds[f\"prognosis_{ind}\"] = pred.prognosis.values\n",
    "            preds[f\"prognosis_{ind}\"].replace('SEVERE', 1, inplace=True)\n",
    "            preds[f\"prognosis_{ind}\"].replace('MILD', 0, inplace=True)\n",
    "            \n",
    "            preds[\"prognosis_real\"] = pred[\"prognosis_real\"].values\n",
    "            preds[\"prognosis_real\"].replace('SEVERE', 1, inplace=True)\n",
    "            preds[\"prognosis_real\"].replace('MILD', 0, inplace=True)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    modes = stats.mode(preds[[f\"prognosis_{ind}\" for ind, _ in enumerate(paths)]].values,\n",
    "                       axis=1)[0]\n",
    "    preds[\"prognosis_mode\"] = modes\n",
    "        \n",
    "    ba_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    \n",
    "    if single_test:\n",
    "        for _ in range(100):\n",
    "            indicies = np.random.choice(len(preds[\"prognosis_real\"].values),\n",
    "                                        size=len(preds[\"prognosis_real\"].values),\n",
    "                                        replace=True)\n",
    "            _reals = preds[\"prognosis_real\"][indicies]\n",
    "            _preds = preds[\"prognosis_0\"][indicies]\n",
    "            ba_scores.append(balanced_accuracy_score(_reals, _preds))\n",
    "            f1_scores.append(f1_score(_reals, _preds))\n",
    "            auc_scores.append(roc_auc_score(_reals, _preds))\n",
    "        \n",
    "        print('CV analysis results')\n",
    "        print('$', np.around(np.mean(ba_scores), 3), '\\pm', np.around(np.std(ba_scores), 3), '$',\n",
    "             \"\\tBA score (full train, on test | BOOTSTRAPPED)\")\n",
    "        print('$', np.around(np.mean(f1_scores), 3), '\\pm', np.around(np.std(f1_scores), 3), '$',\n",
    "             \"\\tF1-score score (full train, on test | BOOTSTRAPPED)\")\n",
    "        print('$', np.around(np.mean(auc_scores), 3), '\\pm', np.around(np.std(auc_scores), 3), '$',\n",
    "             \"\\tAUC score (full train, on test | BOOTSTRAPPED)\")\n",
    "    \n",
    "        return\n",
    "        \n",
    "    for ind, path_dir in enumerate(paths):\n",
    "        if float == type(preds[f\"prognosis_{ind}\"].values[0]):\n",
    "            continue\n",
    "        ba_scores.append(balanced_accuracy_score(preds[\"prognosis_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))\n",
    "        f1_scores.append(f1_score(preds[\"prognosis_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))\n",
    "        auc_scores.append(roc_auc_score(preds[\"prognosis_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))   \n",
    "        \n",
    "    bs_ba_scores = []\n",
    "    for _ in range(100):\n",
    "            indicies = np.random.choice(len(preds[\"prognosis_real\"].values),\n",
    "                                        size=10,\n",
    "                                        replace=True)\n",
    "            _reals = preds[\"prognosis_real\"][indicies].values.astype(int)\n",
    "            _preds = preds[\"prognosis_mode\"][indicies].values.astype(int)\n",
    "            bs_ba_scores.append(balanced_accuracy_score(_reals, _preds))\n",
    "\n",
    "    print('Test analysis results')\n",
    "    space = \" \"\n",
    "    print('$', np.around(np.mean(bs_ba_scores), 3), '\\pm', np.around(np.std(bs_ba_scores), 3), '$',\n",
    "          f'{10*space}BA score (CV test | MODE, BOOTSTRAPPED)')\n",
    "\n",
    "    print('$', np.around(np.mean(ba_scores),3), '\\pm', np.around(np.std(ba_scores), 3), '$',\n",
    "          f'{10*space}Balanced accuracy score (CV test)')\n",
    "    print('$', np.around(np.mean(f1_scores), 3), '\\pm', np.around(np.std(f1_scores), 3), '$',\n",
    "         f\"{10*space}F1-score score (CV test)\")\n",
    "    print('$', np.around(np.mean(auc_scores), 3), '\\pm', np.around(np.std(auc_scores), 3), '$',\n",
    "         f\"{10*space}AUC score (CV test)\")\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b1245a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_analysis(base_path):\n",
    "    paths = list(filter(lambda x: x is not None, [path if path.is_dir() else None for path in\\\n",
    "         Path(base_path).iterdir()]))\n",
    "\n",
    "    file = pd.read_csv(paths[0] / 'pred_valid.csv').file\n",
    "\n",
    "    cols = [\"file\"]\n",
    "    cols.extend([f\"prognosis_{ind}\"for ind in range(len(list(paths)))])\n",
    "    preds = pd.DataFrame(columns=cols)\n",
    "\n",
    "    preds[\"file\"] = file\n",
    "\n",
    "    for ind, path_dir in enumerate(paths):\n",
    "        try:\n",
    "            pred = pd.read_csv(path_dir / 'pred_valid.csv')\n",
    "            preds[f\"prognosis_{ind}\"] = pred.prognosis\n",
    "            preds[f\"prognosis_{ind}_real\"] = pred[\"prognosis_real\"]\n",
    "            preds[f\"prognosis_{ind}\"].replace('SEVERE', 1, inplace=True)\n",
    "            preds[f\"prognosis_{ind}\"].replace('MILD', 0, inplace=True)\n",
    "            preds[f\"prognosis_{ind}_real\"].replace('SEVERE', 1, inplace=True)\n",
    "            preds[f\"prognosis_{ind}_real\"].replace('MILD', 0, inplace=True)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    ba_scores = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "\n",
    "    for ind, path_dir in enumerate(paths):\n",
    "        ba_scores.append(balanced_accuracy_score(preds[f\"prognosis_{ind}_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))\n",
    "        f1_scores.append(f1_score(preds[f\"prognosis_{ind}_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))\n",
    "        auc_scores.append(roc_auc_score(preds[f\"prognosis_{ind}_real\"].values.astype(int),\n",
    "                                              preds[f\"prognosis_{ind}\"].values.astype(int)))\n",
    "\n",
    "    print('CV analysis results')\n",
    "    space = \" \"\n",
    "    print('$', np.around(np.mean(ba_scores),3), '\\pm', np.around(np.std(ba_scores), 3), '$',\n",
    "          f'{10*space}Balanced accuracy score (CV)')\n",
    "    print('$', np.around(np.mean(f1_scores), 3), '\\pm', np.around(np.std(f1_scores), 3), '$',\n",
    "         f\"{10*space}F1-score score (CV)\")\n",
    "    print('$', np.around(np.mean(auc_scores), 3), '\\pm', np.around(np.std(auc_scores), 3), '$',\n",
    "         f\"{10*space}AUC score (CV)\")\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b01fc513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test analysis results\n",
      "$ 0.749 \\pm 0.165 $           BA score (CV test | MODE, BOOTSTRAPPED)\n",
      "$ 0.708 \\pm 0.021 $           Balanced accuracy score (CV test)\n",
      "$ 0.636 \\pm 0.032 $           F1-score score (CV test)\n",
      "$ 0.708 \\pm 0.021 $           AUC score (CV test)\n",
      "CV analysis results\n",
      "$ 0.865 \\pm 0.019 $           Balanced accuracy score (CV)\n",
      "$ 0.864 \\pm 0.021 $           F1-score score (CV)\n",
      "$ 0.865 \\pm 0.019 $           AUC score (CV)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/ai4covid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "## ORIGINAL COMPETITION SUBMISSION --> trained on full training set, many more models to reduce variance in pred.\n",
    "_ = run_test_analysis(base_path='/mnt/ncshare/ai4covid_hackathon/raw_output/checkpoints/ENSEMBLE/')\n",
    "_ = run_cv_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/checkpoints/ENSEMBLE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c605e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test analysis results\n",
      "$ 0.751 \\pm 0.128 $           BA score (CV test | MODE, BOOTSTRAPPED)\n",
      "$ 0.724 \\pm 0.018 $           Balanced accuracy score (CV test)\n",
      "$ 0.663 \\pm 0.022 $           F1-score score (CV test)\n",
      "$ 0.724 \\pm 0.018 $           AUC score (CV test)\n",
      "CV analysis results\n",
      "$ 0.764 \\pm 0.02 $           Balanced accuracy score (CV)\n",
      "$ 0.774 \\pm 0.03 $           F1-score score (CV)\n",
      "$ 0.764 \\pm 0.02 $           AUC score (CV)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/ai4covid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "## PAPER CHKPT, with early stopping enabled --> on a single model basis, better than the first one\n",
    "_ = run_test_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/cross_val/checkpoints/CROSS_VAL_paper_v1_w_ES/')\n",
    "_ = run_cv_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/cross_val/checkpoints/CROSS_VAL_paper_v1_w_ES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b394b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test analysis results\n",
      "$ 0.631 \\pm 0.155 $           BA score (CV test | MODE, BOOTSTRAPPED)\n",
      "$ 0.575 \\pm 0.073 $           Balanced accuracy score (CV test)\n",
      "$ 0.552 \\pm 0.052 $           F1-score score (CV test)\n",
      "$ 0.575 \\pm 0.073 $           AUC score (CV test)\n",
      "CV analysis results\n",
      "$ 0.596 \\pm 0.065 $           Balanced accuracy score (CV)\n",
      "$ 0.665 \\pm 0.035 $           F1-score score (CV)\n",
      "$ 0.596 \\pm 0.065 $           AUC score (CV)\n"
     ]
    }
   ],
   "source": [
    "_ = run_test_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/cross_val/checkpoints_image_only/')\n",
    "_ = run_cv_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/cross_val/checkpoints_image_only/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e9289fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test analysis results\n",
      "$ 0.753 \\pm 0.135 $           BA score (CV test | MODE, BOOTSTRAPPED)\n",
      "$ 0.705 \\pm 0.04 $           Balanced accuracy score (CV test)\n",
      "$ 0.64 \\pm 0.055 $           F1-score score (CV test)\n",
      "$ 0.705 \\pm 0.04 $           AUC score (CV test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/ai4covid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "_ = run_test_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "314cd2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test analysis results\n",
      "$ 0.677 \\pm 0.163 $           BA score (CV test | MODE, BOOTSTRAPPED)\n",
      "$ 0.631 \\pm 0.049 $           Balanced accuracy score (CV test)\n",
      "$ 0.582 \\pm 0.048 $           F1-score score (CV test)\n",
      "$ 0.631 \\pm 0.049 $           AUC score (CV test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/ai4covid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "_ = run_test_analysis('/mnt/ncshare/ai4covid_hackathon/raw_output/checkpoints_image_only/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9d6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
